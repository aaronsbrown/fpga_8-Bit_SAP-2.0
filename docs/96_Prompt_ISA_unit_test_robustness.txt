 I'd like to make a testbench for a CPU microinstruction more robust.

 ISA documentation: docs/hardware/0_ISA.md

 INSTRUCTION_NAME: RAR
 TESTBENCH_FILE: hardware/test/instruction_set/RAR_tb.sv
 ASM_FILE: software/asm/src/RAR.asm

  Please enhance both the assembly program and testbench following these patterns:

  **Assembly Program Enhancements:**
  - Add comprehensive test cases covering edge cases and boundary conditions
  - Include bit pattern variations (all zeros, all ones, alternating patterns, single bits)
  - Test flag behavior scenarios (zero results, negative results, carry interactions)
  - Add register preservation tests to ensure uninvolved registers aren't corrupted
  - Include detailed comments explaining the expected results for each test case

  **Testbench Improvements:**
  - Remove irrelevant signals (like UART) that aren't needed for microinstruction testing
  - Organize tests into clear, numbered sections with descriptive headers
  - Add systematic verification of register values and flag states after each operation
  - Include bit-level explanations in test output (e.g., "$AA & $55 = $00")
  - Use descriptive assertion messages that explain what each check validates
  - Increase timeout values appropriately for the expanded test suite
  - Ensure proper halt verification at the end

  **Key Requirements:**
  - Focus purely on the microinstruction functionality
  - Test both normal operation and edge cases
  - Verify all relevant CPU flags (Zero, Negative, Carry)
  - Confirm register preservation for uninvolved registers
  - Provide clear pass/fail feedback with explanatory messages
  - When updating a testbench, to support correct function of 'run_tests.py', the final log statement *must* be: 
    $display("LDA_I test finished.===========================\n\n");
  

  Please first analyze the current implementation, then enhance both files and regenerate the test
  fixtures. Run the test to verify all improvements work correctly.

  **Implementation Steps:**
  1. After enhancing the assembly file, regenerate test fixtures with:
     ```bash
     python3 software/assembler/src/assembler.py software/asm/src/[ASM_FILE].asm
  hardware/test/fixtures_generated/[INSTRUCTION_NAME]/ --region ROM 0xF000 0xFFFF

  2. Run the enhanced testbench with:
  ./scripts/simulate.sh --tb hardware/test/instruction_set/[TESTBENCH_FILE].sv --verbose --no-viz

  General Worflow guidelinesâ€”You MUST follow this order of operations.
  1. **Consult Relevant Guidance**: When the user gives an instruction, consult the relevant files in the project directory to gather insight
  2. **Clarify Ambiguities**: Based on what you could gather, see if there's any need for clarifications. If so, ask the user targeted questions before proceeding.
  3. **Break Down & Plan**: Break down the task at hand and chalk out a rough plan for carrying it out, referencing project conventions and best practices.
  4. **Trivial Tasks**: If the plan/request is trivial, go ahead and get started immediately.
  5. **Non-Trivial Tasks**: Otherwise, present the plan to the user for review and iterate based on their feedback.
  6. **Track Progress**: Use a to-do list (internally, or optionally in a `TODOS.md` file) to keep track of your progress on multi-step or complex tasks.
  7. **If Stuck, Re-plan**: If you get stuck or blocked, return to step 3 to re-evaluate and adjust your plan.
  8. **Update Documentation**: Once the user's request is fulfilled, update relevant anchor comments (`AIDEV-NOTE`, etc.) and `AGENTS.md` files in the files and directories you touched.
  9. **User Review**: After completing the task, ask the user to review what you've done, and repeat the process as needed.
  10. **Session Boundaries**: If the user's request isn't directly related to the current context and can be safely started in a fresh session, suggest starting from scratch to avoid context confusion.